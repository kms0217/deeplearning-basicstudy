{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "institutional-volleyball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-population",
   "metadata": {},
   "source": [
    "### raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "challenging-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = [\"I'm in the same boat as you.\t나도 너랑 같은 처지야.\",\n",
    "       \"I'm not sure what they want.\t그들이 원하는 게 뭔지 모르겠다.\",\n",
    "       \"I'm not sure where to start.\t어디서부터 시작할지 잘 모르겠어.\",\n",
    "       \"I'm sorry I opened my mouth.\t내가 입을 열어서 미안해.\",\n",
    "       \"I'm sorry if I offended you.\t기분 상하게 했다면 미안해.\",\n",
    "       \"I'm sorry that I kissed Tom.\t톰한테 키스해서 미안해.\",\n",
    "       \"I'm sorry, I can't help you.\t미안한데, 내가 도와줄 수가 없어.\",\n",
    "       \"I'm still in love with Mary.\t나는 아직 메리를 사랑해.\",\n",
    "       \"I've been working all night.\t나는 밤새 일하던 중이었어.\",\n",
    "       \"I've been working very hard.\t난 열심히 일하고 있었어.\",\n",
    "       \"I've learned a lot from Tom.\t난 톰한테서 많은 걸 배웠어.\",\n",
    "       \"Is that all you want to say?\t그게 네가 하고 싶은 말 전부니?\",\n",
    "       \"Is there enough room for us?\t저희들을 위한 방이 충분히 있나요?\",\n",
    "       \"It couldn't have been worse.\t이보다 더 나쁠 수는 없었어.\",\n",
    "       \"It's a symbol of friendship.\t이건 우정의 표시야.\",\n",
    "       \"It's nothing I can't handle.\t내가 감당할 수 있어.\",\n",
    "       \"Jackson fell from his horse.\t잭슨은 자기 말에서 떨어졌어.\",\n",
    "       \"Let us know what you decide.\t네가 결정한 걸 알려줘 봐.\",\n",
    "       \"Mary is a middle-aged woman.\t메리는 중년 여성이야.\",\n",
    "       \"Most people think I'm crazy.\t대부분의 사람들은 내가 미쳤다고 생각해\",\n",
    "       \"My computer broke yesterday.\t어제 내 컴퓨터는 고장났어요.\",\n",
    "       \"My son is small for his age.\t내 아들은 또래에 비해 키가 작아.\",\n",
    "       \"Not all of my cats are gray.\t모든 고양이가 회색인 건 아니야.\",\n",
    "       \"Our new car is not very big.\t우리의 새 차는 그렇게 크지는 않다.\",\n",
    "       \"Perhaps you should tell Tom.\t아마 너는 톰에게 말해야 해.\",\n",
    "       \"Please tidy up your bedroom.\t네 침실 좀 정리해라.\",\n",
    "       \"She doesn't love me anymore.\t그 사람은 나를 더 이상 사랑하지 않아.\",\n",
    "       \"She is watering the flowers.\t그 사람은 꽃에 물을 주고 있어.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-detective",
   "metadata": {},
   "source": [
    "### SOS(start of sentence), EOS(end of sentence)\n",
    "- 문장의 시작 끝을 나타내기 위한 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "occasional-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-wings",
   "metadata": {},
   "source": [
    "### Vocab 클래스\n",
    "- Source와 Target에 사용되는 단어들을 다루기위한 클래스\n",
    "- vocab2index : 단어로 index를 찾기위한 dict\n",
    "- index2vocab : index로 단어를 찾기위한 dict\n",
    "- vocab_count : 각 단어별 갯수\n",
    "- n_vocob : dict에 들어있는 단어 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "literary-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.vocab2index = {\"<SOS>\": SOS_token, \"<EOS>\": EOS_token}\n",
    "        self.index2vocab = {SOS_token: \"<SOS>\", EOS_token: \"<EOS>\"}\n",
    "        self.vocab_count = {}\n",
    "        self.n_vocab = len(self.vocab2index)\n",
    "\n",
    "    def add_vocab(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word not in self.vocab2index:\n",
    "                self.vocab2index[word] = self.n_vocab\n",
    "                self.vocab_count[word] = 1\n",
    "                self.index2vocab[self.n_vocab] = word\n",
    "                self.n_vocab += 1\n",
    "            else:\n",
    "                self.vocab_count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-roads",
   "metadata": {},
   "source": [
    "### filter함수\n",
    "- 최대 단어 개수를 지정해 해당 단어 개수 이하의 문장만 사용하기위한 filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "loved-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the long sentence from source and target data\n",
    "def filter_pair(pair, source_max_length, target_max_length):\n",
    "    return len(pair[0].split(\" \")) < source_max_length and len(pair[1].split(\" \")) < target_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-exhibit",
   "metadata": {},
   "source": [
    "###  data 전처리 함수\n",
    "- tab으로 구분된 영어와 한국어를 하나의 list로 나누고 모든 영어, 한국어 list를 pairs라는 리스트에 추가한다.\n",
    "- 최대 개수를 넘는 pair는 삭제\n",
    "- Vocab()객체를 만들어 조건에 맞는 source, target값을 Vocab.add_vocab를 사용해 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compound-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, source_max_length, target_max_length):\n",
    "    print(\"reading corpus...\")\n",
    "    pairs = []\n",
    "    for line in corpus:\n",
    "        pairs.append([s for s in line.strip().lower().split(\"\\t\")])\n",
    "    print(\"Read {} sentence pairs\".format(len(pairs)))\n",
    "\n",
    "    pairs = [pair for pair in pairs if filter_pair(pair, source_max_length, target_max_length)]\n",
    "    print(\"Trimmed to {} sentence pairs\".format(len(pairs)))\n",
    "\n",
    "    source_vocab = Vocab()\n",
    "    target_vocab = Vocab()\n",
    "\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        source_vocab.add_vocab(pair[0])\n",
    "        target_vocab.add_vocab(pair[1])\n",
    "    print(\"source vocab size =\", source_vocab.n_vocab)\n",
    "    print(\"target vocab size =\", target_vocab.n_vocab)\n",
    "\n",
    "    return pairs, source_vocab, target_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-annex",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "- embedding은 nn.Embedding(총 단어의 갯수, 임베딩 시킬 벡터의 차원)처럼 사용한다.\n",
    "- embedding layer를 거쳐 GRU(RNN)을 통과하는 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "regulation-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size) # RNN Unit\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x).view(1, 1, -1)\n",
    "        x, hidden = self.gru(x, hidden)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-right",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "- Encoder와 큰 차이는 없지만 최종 출력을 만드는 하나의 Layer만 추가됐다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tough-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x).view(1, 1, -1)\n",
    "        x, hidden = self.gru(x, hidden)\n",
    "        x = self.softmax(self.out(x[0]))\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-sandwich",
   "metadata": {},
   "source": [
    "### Tensorize\n",
    "- sentence를 index벡터로 바꿔줌\n",
    "- {\"i\" : 3 , \"love\" : 14, \"you\" : 23}이고, sentence가 \"i love you\" 라면 [3][14][23]를 반환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "independent-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize(vocab, sentence):\n",
    "    indexes = [vocab.vocab2index[word] for word in sentence.split(\" \")]\n",
    "    indexes.append(vocab.vocab2index[\"<EOS>\"])\n",
    "    return torch.Tensor(indexes).long().to(device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-calculation",
   "metadata": {},
   "source": [
    "### training\n",
    "- loss의 경우 negative log likelihood를 사용한다.\n",
    "    - categorical value들끼리의 비교에서 많이 사용한다.\n",
    "    - CorssEntropyLoss를 사용해도 된다.\n",
    "- encoder의 경우 처음 hidden이 없으므로 0으로 초기화 해준 hidden을 넣어준다.\n",
    "- decoder의 경우 encoder의 최종 output을 hidden으로 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "previous-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pairs, source_vocab, target_vocab, encoder, decoder, n_iter, print_every=1000, learning_rate=0.01):\n",
    "    loss_total = 0\n",
    "    \n",
    "    # optimizer\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    training_batch = [random.choice(pairs) for _ in range(n_iter)]\n",
    "    # 각 단어가 index로 매핑된 Tensor\n",
    "    training_source = [tensorize(source_vocab, pair[0]) for pair in training_batch]\n",
    "    training_target = [tensorize(target_vocab, pair[1]) for pair in training_batch]\n",
    "    # loss\n",
    "    criterion = nn.NLLLoss() # negative log likelihood\n",
    "\n",
    "    for i in range(1, n_iter + 1):\n",
    "        # shape : (단어개수, 1)\n",
    "        source_tensor = training_source[i - 1]\n",
    "        target_tensor = training_target[i - 1]\n",
    "        \n",
    "        # 처음 hidden이 없으므로 zeros로 초기화\n",
    "        encoder_hidden = torch.zeros([1, 1, encoder.hidden_size]).to(device)\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        source_length = source_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for enc_input in range(source_length):\n",
    "            _, encoder_hidden = encoder(source_tensor[enc_input], encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.Tensor([[SOS_token]]).long().to(device)\n",
    "        # dcooder의 첫 hidden은 encoder의 마지막 출력\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # teacher forcing\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        loss_iter = loss.item() / target_length\n",
    "        loss_total += loss_iter\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            loss_avg = loss_total / print_every\n",
    "            loss_total = 0\n",
    "            print(\"[{} - {:.2f}%] loss = {:05.4f}\".format(i, i / n_iter * 100, loss_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-bullet",
   "metadata": {},
   "source": [
    "### 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dimensional-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert given sentence to check the training\n",
    "def evaluate(pairs, source_vocab, target_vocab, encoder, decoder, target_max_length):\n",
    "    for pair in pairs:\n",
    "        print(\"영어 : \", pair[0])\n",
    "        print(\"원래 뜻 : \", pair[1])\n",
    "        source_tensor = tensorize(source_vocab, pair[0])\n",
    "        source_length = source_tensor.size()[0]\n",
    "        encoder_hidden = torch.zeros([1, 1, encoder.hidden_size]).to(device)\n",
    "\n",
    "        for ei in range(source_length):\n",
    "            _, encoder_hidden = encoder(source_tensor[ei], encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.Tensor([[SOS_token]]).long().to(device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(target_max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            _, top_index = decoder_output.data.topk(1)\n",
    "            if top_index.item() == EOS_token:\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(target_vocab.index2vocab[top_index.item()])\n",
    "\n",
    "            decoder_input = top_index.squeeze().detach()\n",
    "\n",
    "        predict_words = decoded_words\n",
    "        predict_sentence = \" \".join(predict_words)\n",
    "        print(\"예상 값 : \", predict_sentence)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "banned-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare max length for sentence\n",
    "SOURCE_MAX_LENGTH = 10\n",
    "TARGET_MAX_LENGTH = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "scenic-regular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading corpus...\n",
      "Read 28 sentence pairs\n",
      "Trimmed to 28 sentence pairs\n",
      "Counting words...\n",
      "source vocab size = 106\n",
      "target vocab size = 117\n",
      "[\"i'm sorry, i can't help you.\", '미안한데, 내가 도와줄 수가 없어.']\n"
     ]
    }
   ],
   "source": [
    "# preprocess the corpus\n",
    "load_pairs, load_source_vocab, load_target_vocab = preprocess(raw, SOURCE_MAX_LENGTH, TARGET_MAX_LENGTH)\n",
    "print(random.choice(load_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "inner-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the encoder and the decoder\n",
    "enc_hidden_size = 16\n",
    "dec_hidden_size = enc_hidden_size\n",
    "enc = Encoder(load_source_vocab.n_vocab, enc_hidden_size).to(device)\n",
    "dec = Decoder(dec_hidden_size, load_target_vocab.n_vocab).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "occupational-consultancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000 - 5.00%] loss = 3.7395\n",
      "[2000 - 10.00%] loss = 2.3175\n",
      "[3000 - 15.00%] loss = 1.3137\n",
      "[4000 - 20.00%] loss = 0.8308\n",
      "[5000 - 25.00%] loss = 0.5517\n",
      "[6000 - 30.00%] loss = 0.3939\n",
      "[7000 - 35.00%] loss = 0.2866\n",
      "[8000 - 40.00%] loss = 0.2205\n",
      "[9000 - 45.00%] loss = 0.1722\n",
      "[10000 - 50.00%] loss = 0.1382\n",
      "[11000 - 55.00%] loss = 0.1167\n",
      "[12000 - 60.00%] loss = 0.0959\n",
      "[13000 - 65.00%] loss = 0.0851\n",
      "[14000 - 70.00%] loss = 0.0756\n",
      "[15000 - 75.00%] loss = 0.0684\n",
      "[16000 - 80.00%] loss = 0.0591\n",
      "[17000 - 85.00%] loss = 0.0540\n",
      "[18000 - 90.00%] loss = 0.0487\n",
      "[19000 - 95.00%] loss = 0.0454\n",
      "[20000 - 100.00%] loss = 0.0413\n"
     ]
    }
   ],
   "source": [
    "train(load_pairs, load_source_vocab, load_target_vocab, enc, dec, 20000, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "classical-thought",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 :  i'm in the same boat as you.\n",
      "원래 뜻 :  나도 너랑 같은 처지야.\n",
      "예상 값 :  나도 너랑 같은 처지야. <EOS>\n",
      "\n",
      "영어 :  i'm not sure what they want.\n",
      "원래 뜻 :  그들이 원하는 게 뭔지 모르겠다.\n",
      "예상 값 :  그들이 원하는 게 뭔지 모르겠다. <EOS>\n",
      "\n",
      "영어 :  i'm not sure where to start.\n",
      "원래 뜻 :  어디서부터 시작할지 잘 모르겠어.\n",
      "예상 값 :  어디서부터 시작할지 잘 모르겠어. <EOS>\n",
      "\n",
      "영어 :  i'm sorry i opened my mouth.\n",
      "원래 뜻 :  내가 입을 열어서 미안해.\n",
      "예상 값 :  내가 입을 열어서 미안해. <EOS>\n",
      "\n",
      "영어 :  i'm sorry if i offended you.\n",
      "원래 뜻 :  기분 상하게 했다면 미안해.\n",
      "예상 값 :  기분 상하게 했다면 미안해. <EOS>\n",
      "\n",
      "영어 :  i'm sorry that i kissed tom.\n",
      "원래 뜻 :  톰한테 키스해서 미안해.\n",
      "예상 값 :  톰한테 키스해서 미안해. <EOS>\n",
      "\n",
      "영어 :  i'm sorry, i can't help you.\n",
      "원래 뜻 :  미안한데, 내가 도와줄 수가 없어.\n",
      "예상 값 :  미안한데, 내가 도와줄 수가 없어. <EOS>\n",
      "\n",
      "영어 :  i'm still in love with mary.\n",
      "원래 뜻 :  나는 아직 메리를 사랑해.\n",
      "예상 값 :  나는 아직 메리를 사랑해. <EOS>\n",
      "\n",
      "영어 :  i've been working all night.\n",
      "원래 뜻 :  나는 밤새 일하던 중이었어.\n",
      "예상 값 :  나는 밤새 일하던 중이었어. <EOS>\n",
      "\n",
      "영어 :  i've been working very hard.\n",
      "원래 뜻 :  난 열심히 일하고 있었어.\n",
      "예상 값 :  난 열심히 일하고 있었어. <EOS>\n",
      "\n",
      "영어 :  i've learned a lot from tom.\n",
      "원래 뜻 :  난 톰한테서 많은 걸 배웠어.\n",
      "예상 값 :  난 톰한테서 많은 걸 배웠어. <EOS>\n",
      "\n",
      "영어 :  is that all you want to say?\n",
      "원래 뜻 :  그게 네가 하고 싶은 말 전부니?\n",
      "예상 값 :  그게 네가 하고 싶은 말 전부니? <EOS>\n",
      "\n",
      "영어 :  is there enough room for us?\n",
      "원래 뜻 :  저희들을 위한 방이 충분히 있나요?\n",
      "예상 값 :  저희들을 위한 방이 충분히 있나요? <EOS>\n",
      "\n",
      "영어 :  it couldn't have been worse.\n",
      "원래 뜻 :  이보다 더 나쁠 수는 없었어.\n",
      "예상 값 :  이보다 더 나쁠 수는 없었어. <EOS>\n",
      "\n",
      "영어 :  it's a symbol of friendship.\n",
      "원래 뜻 :  이건 우정의 표시야.\n",
      "예상 값 :  이건 우정의 표시야. <EOS>\n",
      "\n",
      "영어 :  it's nothing i can't handle.\n",
      "원래 뜻 :  내가 감당할 수 있어.\n",
      "예상 값 :  내가 감당할 수 있어. <EOS>\n",
      "\n",
      "영어 :  jackson fell from his horse.\n",
      "원래 뜻 :  잭슨은 자기 말에서 떨어졌어.\n",
      "예상 값 :  잭슨은 자기 말에서 떨어졌어. <EOS>\n",
      "\n",
      "영어 :  let us know what you decide.\n",
      "원래 뜻 :  네가 결정한 걸 알려줘 봐.\n",
      "예상 값 :  네가 결정한 걸 알려줘 봐. <EOS>\n",
      "\n",
      "영어 :  mary is a middle-aged woman.\n",
      "원래 뜻 :  메리는 중년 여성이야.\n",
      "예상 값 :  메리는 중년 여성이야. <EOS>\n",
      "\n",
      "영어 :  most people think i'm crazy.\n",
      "원래 뜻 :  대부분의 사람들은 내가 미쳤다고 생각해\n",
      "예상 값 :  대부분의 사람들은 내가 미쳤다고 생각해 <EOS>\n",
      "\n",
      "영어 :  my computer broke yesterday.\n",
      "원래 뜻 :  어제 내 컴퓨터는 고장났어요.\n",
      "예상 값 :  어제 내 컴퓨터는 고장났어요. <EOS>\n",
      "\n",
      "영어 :  my son is small for his age.\n",
      "원래 뜻 :  내 아들은 또래에 비해 키가 작아.\n",
      "예상 값 :  내 아들은 또래에 비해 키가 작아. <EOS>\n",
      "\n",
      "영어 :  not all of my cats are gray.\n",
      "원래 뜻 :  모든 고양이가 회색인 건 아니야.\n",
      "예상 값 :  모든 고양이가 회색인 건 아니야. <EOS>\n",
      "\n",
      "영어 :  our new car is not very big.\n",
      "원래 뜻 :  우리의 새 차는 그렇게 크지는 않다.\n",
      "예상 값 :  우리의 새 차는 그렇게 크지는 않다. <EOS>\n",
      "\n",
      "영어 :  perhaps you should tell tom.\n",
      "원래 뜻 :  아마 너는 톰에게 말해야 해.\n",
      "예상 값 :  아마 너는 톰에게 말해야 해. <EOS>\n",
      "\n",
      "영어 :  please tidy up your bedroom.\n",
      "원래 뜻 :  네 침실 좀 정리해라.\n",
      "예상 값 :  네 침실 좀 정리해라. <EOS>\n",
      "\n",
      "영어 :  she doesn't love me anymore.\n",
      "원래 뜻 :  그 사람은 나를 더 이상 사랑하지 않아.\n",
      "예상 값 :  그 사람은 나를 더 이상 사랑하지 않아. <EOS>\n",
      "\n",
      "영어 :  she is watering the flowers.\n",
      "원래 뜻 :  그 사람은 꽃에 물을 주고 있어.\n",
      "예상 값 :  그 사람은 꽃에 물을 주고 있어. <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(load_pairs, load_source_vocab, load_target_vocab, enc, dec, TARGET_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-library",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
