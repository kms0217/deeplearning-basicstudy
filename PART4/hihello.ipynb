{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "political-plasma",
   "metadata": {},
   "source": [
    "### input data 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "static-worship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data : [[4, 1, 4, 3, 2, 2]]\n",
      "y_data : [[1, 4, 3, 2, 2, 0]]\n",
      "x_one_hot : [[[0. 0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "sample = \"hihello\"\n",
    "\n",
    "char_set = list(set(sample)) # 중복 제거를 위해 set을 사용해서 위의 표와는 다른 index들이 나옴\n",
    "input_size = len(char_set)\n",
    "hidden_size = len(char_set)\n",
    "learning_rate = 0.1\n",
    "\n",
    "input_sequence = sample[:-1]\n",
    "output_sequence = sample[1:]\n",
    "\n",
    "# input, output sequence to char_set index\n",
    "x_data = [list(map(char_set.index, input_sequence))]\n",
    "y_data = [list(map(char_set.index, output_sequence))]\n",
    "\n",
    "# x_data one hotencoding\n",
    "targets = np.array(x_data)\n",
    "x_one_hot = np.eye(input_size)[targets] # list indexing\n",
    "\n",
    "print(f\"x_data : {x_data}\")\n",
    "print(f\"y_data : {y_data}\")\n",
    "print(f\"x_one_hot : {x_one_hot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-compact",
   "metadata": {},
   "source": [
    "### To tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pressing-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-debate",
   "metadata": {},
   "source": [
    "### model, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brutal-timing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RNN\n",
    "# batch_frist옵션으로 가장 앞 dim이 batch_size가 되도록\n",
    "rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "# loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "complimentary-transsexual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0/100 : loss : 1.5556267499923706, result = ooholl\n",
      " 1/100 : loss : 1.43287193775177, result = oololl\n",
      " 2/100 : loss : 1.323917269706726, result = lollll\n",
      " 3/100 : loss : 1.215421199798584, result = illlll\n",
      " 4/100 : loss : 1.1222037076950073, result = ihelll\n",
      " 5/100 : loss : 1.0601412057876587, result = ihelll\n",
      " 6/100 : loss : 1.0056649446487427, result = ihelll\n",
      " 7/100 : loss : 0.9549684524536133, result = ihelll\n",
      " 8/100 : loss : 0.9277551174163818, result = ihelll\n",
      " 9/100 : loss : 0.8953915238380432, result = ihelll\n",
      " 10/100 : loss : 0.8703381419181824, result = ihelll\n",
      " 11/100 : loss : 0.8513872623443604, result = ihelll\n",
      " 12/100 : loss : 0.8328230977058411, result = ihelll\n",
      " 13/100 : loss : 0.8206679821014404, result = ihelll\n",
      " 14/100 : loss : 0.7994511723518372, result = ihelll\n",
      " 15/100 : loss : 0.7981094717979431, result = ihelll\n",
      " 16/100 : loss : 0.7796568870544434, result = ihelll\n",
      " 17/100 : loss : 0.7748346924781799, result = ihelll\n",
      " 18/100 : loss : 0.7538755536079407, result = ihelll\n",
      " 19/100 : loss : 0.7495420575141907, result = ihelll\n",
      " 20/100 : loss : 0.7256560921669006, result = ihelll\n",
      " 21/100 : loss : 0.7175724506378174, result = ihelll\n",
      " 22/100 : loss : 0.7159164547920227, result = ihelll\n",
      " 23/100 : loss : 0.7051181793212891, result = ihelll\n",
      " 24/100 : loss : 0.6934192180633545, result = ihelll\n",
      " 25/100 : loss : 0.6801701188087463, result = ihelll\n",
      " 26/100 : loss : 0.6733071208000183, result = ihelll\n",
      " 27/100 : loss : 0.6714460253715515, result = ihelll\n",
      " 28/100 : loss : 0.6593523025512695, result = ihelll\n",
      " 29/100 : loss : 0.6504608988761902, result = ihelll\n",
      " 30/100 : loss : 0.6421286463737488, result = ihelll\n",
      " 31/100 : loss : 0.6332640051841736, result = ihelll\n",
      " 32/100 : loss : 0.6312534213066101, result = ihelll\n",
      " 33/100 : loss : 0.6232877373695374, result = ihelll\n",
      " 34/100 : loss : 0.6132873892784119, result = ihelll\n",
      " 35/100 : loss : 0.6079123616218567, result = ihelll\n",
      " 36/100 : loss : 0.6037988066673279, result = ihelll\n",
      " 37/100 : loss : 0.5972568392753601, result = ihelll\n",
      " 38/100 : loss : 0.5908097624778748, result = ihelll\n",
      " 39/100 : loss : 0.5851401686668396, result = ihelll\n",
      " 40/100 : loss : 0.5807684063911438, result = ihelll\n",
      " 41/100 : loss : 0.5771527886390686, result = ihelll\n",
      " 42/100 : loss : 0.5721285343170166, result = ihelll\n",
      " 43/100 : loss : 0.5672069191932678, result = ihelll\n",
      " 44/100 : loss : 0.5637668967247009, result = ihelll\n",
      " 45/100 : loss : 0.5607499480247498, result = ihelll\n",
      " 46/100 : loss : 0.5573720932006836, result = ihelll\n",
      " 47/100 : loss : 0.553982675075531, result = ihelll\n",
      " 48/100 : loss : 0.5514636635780334, result = ihelll\n",
      " 49/100 : loss : 0.5496075749397278, result = ihelll\n",
      " 50/100 : loss : 0.5472718477249146, result = ihelll\n",
      " 51/100 : loss : 0.5451152324676514, result = ihelll\n",
      " 52/100 : loss : 0.5436980724334717, result = ihelll\n",
      " 53/100 : loss : 0.5424060225486755, result = ihelll\n",
      " 54/100 : loss : 0.5408948063850403, result = ihelll\n",
      " 55/100 : loss : 0.5395157933235168, result = ihelll\n",
      " 56/100 : loss : 0.5386198163032532, result = ihelll\n",
      " 57/100 : loss : 0.5377477407455444, result = ihelll\n",
      " 58/100 : loss : 0.5366550087928772, result = ihelll\n",
      " 59/100 : loss : 0.5358389616012573, result = ihelll\n",
      " 60/100 : loss : 0.5352492928504944, result = ihelll\n",
      " 61/100 : loss : 0.5345557928085327, result = ihelll\n",
      " 62/100 : loss : 0.5338109135627747, result = ihelll\n",
      " 63/100 : loss : 0.5332762598991394, result = ihelll\n",
      " 64/100 : loss : 0.5328484773635864, result = ihelll\n",
      " 65/100 : loss : 0.532287061214447, result = ihelll\n",
      " 66/100 : loss : 0.5317931771278381, result = ihelll\n",
      " 67/100 : loss : 0.5314496755599976, result = ihelll\n",
      " 68/100 : loss : 0.5310752391815186, result = ihelll\n",
      " 69/100 : loss : 0.5306499600410461, result = ihelll\n",
      " 70/100 : loss : 0.5303187370300293, result = ihelll\n",
      " 71/100 : loss : 0.5300551652908325, result = ihelll\n",
      " 72/100 : loss : 0.5297264456748962, result = ihelll\n",
      " 73/100 : loss : 0.5294203162193298, result = ihelll\n",
      " 74/100 : loss : 0.5291932225227356, result = ihelll\n",
      " 75/100 : loss : 0.5289508700370789, result = ihelll\n",
      " 76/100 : loss : 0.5286825895309448, result = ihelll\n",
      " 77/100 : loss : 0.5284655094146729, result = ihelll\n",
      " 78/100 : loss : 0.5282765626907349, result = ihelll\n",
      " 79/100 : loss : 0.5280541777610779, result = ihelll\n",
      " 80/100 : loss : 0.5278510451316833, result = ihelll\n",
      " 81/100 : loss : 0.5276848673820496, result = ihelll\n",
      " 82/100 : loss : 0.5275048613548279, result = ihelll\n",
      " 83/100 : loss : 0.5273185968399048, result = ihelll\n",
      " 84/100 : loss : 0.5271633863449097, result = ihelll\n",
      " 85/100 : loss : 0.5270125269889832, result = ihelll\n",
      " 86/100 : loss : 0.5268474817276001, result = ihelll\n",
      " 87/100 : loss : 0.5267007350921631, result = ihelll\n",
      " 88/100 : loss : 0.5265668034553528, result = ihelll\n",
      " 89/100 : loss : 0.5264226794242859, result = ihelll\n",
      " 90/100 : loss : 0.5262841582298279, result = ihelll\n",
      " 91/100 : loss : 0.5261611342430115, result = ihelll\n",
      " 92/100 : loss : 0.5260332822799683, result = ihelll\n",
      " 93/100 : loss : 0.5259047150611877, result = ihelll\n",
      " 94/100 : loss : 0.5257888436317444, result = ihelll\n",
      " 95/100 : loss : 0.5256734490394592, result = ihelll\n",
      " 96/100 : loss : 0.5255548357963562, result = ihelll\n",
      " 97/100 : loss : 0.5254448652267456, result = ihelll\n",
      " 98/100 : loss : 0.5253386497497559, result = ihelll\n",
      " 99/100 : loss : 0.5252291560173035, result = ihelll\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output, _status = rnn(X)\n",
    "    loss = criterion(output.view(-1, input_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    result = output.data.numpy().argmax(axis = 2)\n",
    "    result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
    "    print(f\" {epoch}/{epochs} : loss : {loss.item()}, result = {result_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-class",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
