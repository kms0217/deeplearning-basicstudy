# Vector Matrix Tensor

- 2D Tensor

    $`|t| = (batch size, dim)`$

- 3D Tensor (Typical computer vision)
$`|t| = (batch size, width, height)`$

![image/tensor.png](image/tensor.png)

NLPì—ì„œ ìƒê°í•´ë³´ë©´ width * height ì‚¬ê°í˜•ì´ í•˜ë‚˜ì˜ ë¬¸ì¥ì´ë¼ê³  ìƒê°í•˜ê³  ê·¸ ë¬¸ì¥ì´ batch sizeë§Œí¼ ì¡´ì¬í•œë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤.

---
# [Pytorch_basic](https://github.com/xcvdv/deeplearning-basicstudy/blob/main/PART1/pytorch_basic.ipynb)

PyTorchëŠ” numpyì™€ ë§¤ìš° ìœ ì‚¬í•˜ê³  í˜¸í™˜ì„±ë„ ë†’ë‹¤.

```python
import torch

t = torch.FloatTensor([[0., 1., 2.,],[ 3., 4., 5.],[ 6.,7.,8.,]])

print(t.dim())
print(t.shape)
print(t.size())
```

dimì€ numpy ndimì™€ ê°™ë‹¤. numpyì—ì„œ sizeëŠ” ì „ì²´ ì›ì†Œì˜ ê°œìˆ˜ì˜€ì§€ë§Œ pytorchì˜ sizeëŠ” shapeì™€ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤. ìŠ¬ë¼ì´ì‹±ì€ numpyì™€ ê°™ë‹¤.

---

### í–‰ë ¬ ê³±

numpyì—ì„œ í–‰ë ¬ê³±ì€ @ì—°ì‚°ì í˜¹ì€ dot()í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. pytorchì—ì„œëŠ” matmul()í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. * í˜¹ì€ mul()ì€ numpyì²˜ëŸ¼ element-wise ì—°ì‚°ìœ¼ë¡œ ê³±ì…ˆì´ ìˆ˜í–‰ëœë‹¤.

```python
import torch

m1 = torch.FloatTensor([[0., 1., 2.,],[ 3., 4., 5.],[ 6.,7.,8.,]])
m2 = torch.FloatTensor([[0,1],[2,3],[3,4]])
m1.matmul(m2) # í–‰ë ¬ê³±

```

---

### Mean

mean()í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í‰ê· ì„ êµ¬í•  ìˆ˜ ìˆëŠ”ë° longTensorì—ì„œëŠ” ì œëŒ€ë¡œ ìˆ˜í–‰ë˜ì§€ ì•ŠëŠ”ë‹¤. dimì€ numpyì˜ axisì™€ ê°™ì€ ì—­í• ì„ í•œë‹¤.

```python
t = torch.FloatTensor([[1,2],[3,4]])
t.mean()
t.mean(dim=0) # 0ë²ˆì¶• ê¸°ì¤€ìœ¼ë¡œ meanê³„ì‚°
t.mean(dim=1) # 1ë²ˆì¶• ê¸°ì¤€ìœ¼ë¡œ meanê³„ì‚°
t.mean(dim=-1) # ë§ˆì§€ë§‰ ì¶• ê¸°ì¤€ìœ¼ë¡œ meanê³„ì‚°
```

---

### view

view í•¨ìˆ˜ëŠ” numpyì˜ reshapeí•¨ìˆ˜ì™€ ê°™ì€ ì—­í• ì„ í•œë‹¤.

---

### squeeze

squeezeëŠ” ì›ì†Œ ê°œìˆ˜ê°€ í•˜ë‚˜ì¸ ì°¨ì›ì„ ì œê±°í•´ì¤€ë‹¤.

```python
import torch

f = torch.FloatTensor([[1],[2],[3]])
print(f.shape)
f = f.squeeze()
print(f)
print(f.shape)
```

squeeze(dim=3)ê³¼ ê°™ì´ dimensionì„ ëª…ì‹œí•´ì£¼ë©´ í•´ë‹¹ dimensionì˜ ì›ì†Œ ê°œìˆ˜ê°€ 1ì¼ê²½ìš° ì—†ì–´ì§€ê³  ì•„ë‹Œ ê²½ìš° ì•„ë¬´ëŸ° ë³€í™”ê°€ ì¼ì–´ë‚˜ì§€ ì•ŠìŒ

---

### unsqueeze

ì›í•˜ëŠ” dimensionì— 1ì„ ë„£ì–´ì¤€ë‹¤. squeezeì™€ ë‹¤ë¥´ê²Œ dimensionì„ ë°˜ë“œì‹œ ëª…ì‹œí•´ì¤˜ì•¼ í•œë‹¤.

```python
import torch

f = torch.FloatTensor([1,2,3])
print(f.shape)
f1 = f.unsqueeze(0)
print(f1.shape)
print(f1)
print(f2.shape)
print(f2)
```

---

### í˜•ë³€í™˜

```python
import torch

a = torch.LongTensor([1,2,3,4,5])
b = torch.ByteTensor([True, False])
print(a.float().type())
print(b.float().type())
print(a.byte().type())
print(b.long().type())
```

---

### concatenate

numpyì˜ concatenateì™€ ë˜‘ê°™ë‹¤. `torch.cat([x,y,], dim=0)` ê³¼ ê°™ì´ ì‚¬ìš©í•œë‹¤.

---

### stack

stackí•¨ìˆ˜ëŠ” parameterë¡œ tensorì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ëŠ”ë‹¤.

```python
import torch

x = torch.FloatTensor([1,2])
y = torch.FloatTensor([3,4])
z = torch.FloatTensor([5,6])

print(torch.stack([x,y,z]))
print(torch.stack([x,y,z], dim=1))
```

ë§Œì•½ ìœ„ì˜ ì½”ë“œë¥¼ concatenateë¡œ í•˜ë ¤ë©´ x,y,zë¥¼ unsqueezeë¥¼ ì‚¬ìš©í•œ ë’¤ concatenateí•´ì•¼í•œë‹¤.

---

### likeí•¨ìˆ˜

numpyì™€ ë§ˆì°¬ê°€ì§€ë¡œ `torch.ones_like(tensor), torch.zeros_like(tensor)` ì²˜ëŸ¼ ì‚¬ìš©í•˜ë©´ tensorê³¼ ë˜‘ê°™ì€ ëª¨ì–‘ì˜ ë°°ì—´ì„ ë§Œë“¤ê³  ê°ê° 1ê³¼ 0ìœ¼ë¡œ ì±„ì›Œì¤€ë‹¤. ê°™ì€ deviceì— ìˆì–´ì•¼ (cpuë©´ cpu gpuë©´ ê°™ì€gpu... ë‹¤ë¥¼ê²½ìš° error) ê³„ì‚°ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•œë‹¤. likeí•¨ìˆ˜ëŠ” ê°™ì€ deviceì— í• ë‹¹ëœë‹¤.

---

### in-place operation

underscoreë¥¼ ì‚¬ìš©í•´ ì›ë³¸ ë°°ì—´ì„ ì—°ì‚° ê²°ê³¼ë¡œ ë³€ê²½í•´ì¤€ë‹¤. ìƒˆë¡œ ë©”ëª¨ë¦¬ í• ë‹¹í•˜ì§€ ì•Šê³  ì—°ì‚° ê²°ê³¼ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì†ë„ ë©´ì—ì„œ ì´ë“ì„ ë³¼ê²ƒ ê°™ì§€ë§Œ pytorchì˜ GCê°€ íš¨ìœ¨ì ìœ¼ë¡œ ì˜ êµ¬í˜„ë˜ì–´ ìˆì–´ì„œ ì†ë„ìƒì˜ ì´ì ì€ ë³„ë¡œ ì—†ë‹¤ê³  í•œë‹¤.

```python
import torch

x = torch.FloatTensor([[1,2],[3,4,]])
x.mul(2) # ì›ë³¸ xëŠ” ë³€í™” ì—†ì´ ê²°ê³¼ ë°˜í™˜ë§Œ ë¨
print(x)
x.mul_(2) # ì›ë³¸ xìì²´ê°€ ë³€í•¨
print(x)
```

---
# [í•™ìŠµìœ¼ë¡œ y = Wxì˜ W êµ¬í•˜ê¸°](https://github.com/xcvdv/deeplearning-basicstudy/blob/main/PART1/WX_linear_regression.ipynb)

pytorchì˜ optimizerë¥¼ ì‚¬ìš©í•œ ì½”ë“œì™€ ì‚¬ìš©í•˜ì§€ ì•Šì€ ë‘ê°€ì§€ ì½”ë“œë¥¼ ì‘ì„±í•˜ì˜€ë‹¤.

$`cost(W) = \frac{1}{m}âˆ‘(H(Wx^{(i)}-y^{(i)})^2`$

$`\nabla{W} = \frac{\partial{cost}}{\partial{W}}=\frac{2}{m}\Sigma(Wx^{(i)}-y^{(i)})x^{(i)}`$

í•™ìŠµì„ í†µí•´ Costê°€ ì¤„ì–´ë“¤ë©° Wê°’ì´ êµ¬í•´ì§€ëŠ”ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python
optimizer = optim.SGD([W], lr=0.15) # Optimizer ì •ì˜

# costë¡œ H(x)ê°œì„ 
optimizer.zero_grad() # gradientë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
cost.backward() # gradientê³„ì‚°
optimizer.step() # gradient descent
```

pytorchì˜ optimizerë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–¤ optimizerë¥¼ ì‚¬ìš©í• ì§€ ì„ íƒí•˜ê³ ,

gradientë¥¼ ê³„ì‚°í•˜ëŠ” backward(), ê³„ì‚°ëœ gradientë¡œ Parameterë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” step()ë¥¼ ì‚¬ìš©í•´ì•¼í•œë‹¤.

ë§¤ë²ˆ zero_grad()ë¥¼ í˜¸ì¶œí•˜ëŠ” ì´ìœ ëŠ” gradientë¥¼ ëˆ„ì ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì— ë§¤ë²ˆ 0ìœ¼ë¡œ ì´ˆê¸°í™” í•´ì£¼ëŠ”ê²ƒì´ë‹¤.

---
# [ì…ë ¥ì´ ì—¬ëŸ¬ê°œì¸ Multivariate Linear Regression](https://github.com/xcvdv/deeplearning-basicstudy/blob/main/PART1/Multivariate_linear_regression.ipynb)

ì˜ˆë¥¼ë“¤ì–´ í•™ìƒì˜ 3ë²ˆìœ„ í€´ì¦ˆ ì ìˆ˜ê°€ inputìœ¼ë¡œ ë“¤ì–´ì˜¬ ë•Œ ê¸°ë§ê³ ì‚¬ ì ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“ ë‹¤ê³  ìƒê°í•´ë³´ì.

- dataset
  
    ```python
    # í•™ìƒ 5ëª…ì˜ 3ë²ˆì˜ í€´ì¦ˆ ê²°ê³¼
    x_train = torch.FloatTensor([[73, 80, 75],[93,88,93],[89,91,90],[96,98,100],[73,66,70]])

    # í•™ìƒ 5ëª…ì˜ ê¸°ë§ê³ ì‚¬ ì ìˆ˜
	y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])
    ```

- Hypothesis Function
  
    xì˜ ì •ë¶€ê°€ 3ê°œì´ë¯€ë¡œ Weightë¥¼ 3ê°œ ê°€ì§€ëŠ” í•¨ìˆ˜ë¡œ í‘œí˜„í•´ì•¼í•œë‹¤.

    $`H(x) = w_{1}x_{1}+w_{2}x_{2}+w_{3}x_{3}+b`$
    ```python
    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b
    ```

- Cost
  
    MSEë¥¼ ì‚¬ìš©

---

### classë§Œë“¤ê¸°

ë‚´ìš© ì¶”ê°€ ì˜ˆì •

---
# [Loading Data](https://github.com/xcvdv/deeplearning-basicstudy/blob/main/PART1/Loading_data.ipynb)

- ë°ì´í„°ì˜ ì–‘ì´ ë§ìœ¼ë©´ ëª¨ë¸ì´ ë” ê²¬ê³ í•œ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆê²Œëœë‹¤.
- í•˜ì§€ë§Œ GDë¥¼ í•˜ë ¤ë©´ ê° ë°ì´í„°ë§ˆë‹¤ Costë¥¼ êµ¬í•´ì•¼í•˜ëŠ”ë° ë°ì´í„°ê°€ ë§ì„ê²½ìš° ì—°ì‚°ì†ë„ê°€ ëŠë ¤ì§€ê±°ë‚˜ ì»´í“¨í„°ì— ì €ì¥í•˜ì§€ ëª»í• ìˆ˜ë„ìˆë‹¤.
- ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸°ìœ„í•´ ì „ì²´ ë°ì´í„°ë¥¼ minibatchë¡œ ë‚˜ëˆ„ì–´ ê° minibatchì— ìˆëŠ” Costë§Œ ê³„ì‚°í•´ GDë¥¼ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— ì—…ë°ì´íŠ¸ë¥¼ ì¢€ ë” ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì „ì²´ ë°ì´í„°ë¥¼ ì“°ì§€ ì•Šê³  Costë¥¼ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì— ì˜ëª»ëœ ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë  ìˆ˜ ìˆë‹¤.
- Dataset ë§Œë“¤ê¸°
```python
# pytorch Datasetì„ ìƒì†ë°›ëŠ” classë¥¼ ë§Œë“¤ì–´ ì›í•˜ëŠ” Datasetë§Œë“¤ê¸°
# __len__, __getitem__ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ì•¼í•œë‹¤.
from torch.utils.data import Dataset

class CustomDataset(Dataset):
	def __init__(self):
		self.x_data = [[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]]
		self.y_data = [[152], [1], [152], [152], [152]]
	
	# Datasetì˜ ì´ ê°¯ìˆ˜
	def __len__(self):
		return len(self.x_data)

	# í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ì…ì¶œë ¥ ë°ì´í„° ë°˜í™˜
	def __getitem__(self, idx):
		x = torch.FloatTensor(self.x_data[idx])
		y = torch.FloatTensor(self.y_data[idx])
		return x, y

dataset = CustomDataset()

```

- DataLoader
Datasetì„ ìƒì„±í–ˆìœ¼ë©´ pytorchì˜ Dataloaderë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

dataloader instanceë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” Datasetê³¼ ê° minibatchì˜ í¬ê¸°ë¥¼ ì§€ì •í•´ì•¼í•œë‹¤.

(ì´ë•Œ minibatchí¬ê¸°ëŠ” í†µìƒì ìœ¼ë¡œ 2ì˜ ì œê³±ìˆ˜ë¡œ ì„¤ì •í•œë‹¤.)

shuffleì˜µì…˜ì„ Trueë¡œ í•´ì£¼ì–´ Epochë§ˆë‹¤ ë°ì´í„°ì…‹ì„ ì„ì–´ ë°ì´í„°ê°€ í•™ìŠµë˜ëŠ” ìˆœì„œë¥¼ ë°”ê¿€ ìˆ˜ ìˆë‹¤.

```python
# pytorchì˜ DataLoaderë¥¼ ì‚¬ìš©í•´ Datasetì„ ì›í•˜ëŠ” ì„¤ì •ì„ ì½ì„ ìˆ˜ ìˆë‹¤.
from torch.utils.data import DataLoader

# DataLoader instanceë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” Dataset, ê³¼ minibatchí¬ê¸°ë¥¼ ì§€ì •í•´ì•¼í•œë‹¤.
# shuffleì˜µì…˜ì„ ì‚¬ìš©í•´ Epochë§ˆë‹¤ ë°ì´í„°ì…‹ì„ ì„ì–´ì„œ í•™ìŠµ ìˆœì„œë¥¼ ë°”ê¿€ ìˆ˜ ìˆë‹¤.
Dataloader = DataLoader(
	dataset,
	batch_size=2
	shuffle=True,
)

```
# [Logistic Regression](https://github.com/xcvdv/deeplearning-basicstudy/blob/main/PART1/Logistic_regression.ipynb)

- í”íˆ ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ì¢…ì†ë³€ìˆ˜ê°€ ì´í•­í˜• ë¬¸ì œë¥¼ ì§€ì¹­í•  ë•Œ ì‚¬ìš©ëœë‹¤. ì´ì™¸ì—, ë‘ ê°œ ì´ìƒì˜ ë²”ì£¼ë¥¼ ê°€ì§€ëŠ” ë¬¸ì œê°€ ëŒ€ìƒì¸ ê²½ìš°ì—” ë‹¤í•­ ë¡œì§€ìŠ¤í‹± íšŒê·€ ë˜ëŠ” ë¶„í™” ë¡œì§€ìŠ¤í‹± íšŒê·€ (polytomous logistic regression)ë¼ê³  í•˜ê³  ë³µìˆ˜ì˜ ë²”ì£¼ì´ë©´ì„œ ìˆœì„œê°€ ì¡´ì¬í•˜ë©´ ì„œìˆ˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ (ordinal logistic regression) ë¼ê³  í•œë‹¤
- ì‹¤ì œ ë‚˜ì™€ì•¼í•˜ëŠ” yì™€ ì˜ˆì¸¡ê°’ì´ yì¼ í™•ë¥ ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ Wë¥¼ ì—…ë°ì´íŠ¸ í•˜ë©´ ëœë‹¤.
- ë³´í†µ ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œëŠ” ë§ˆì§€ë§‰ ì¶œë ¥ì„ 0 ~ 1ì‚¬ì´ë¡œ ë‚˜ì˜¤ë„ë¡ í•˜ê¸°ìœ„í•´ Sigmoidí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.
- ê°„ë‹¨í•˜ê²Œ Weightê°€ í•˜ë‚˜ì¸ ëª¨ë¸ì„ ìƒê°í•´ë³´ë©´ ì•„ë˜ì™€ê°™ì€ Hypothesisì™€ costë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.
- Hypothesis

    Wê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì–´ë–¤í•œ ìƒ˜í”Œ Xê°€ 1ì´ë  í™•ë¥  P(X=1; W)

    $$```H(X) = \frac{1}{1 + e^{-XW}}```$$

- cost

    $$```cost(W)=\frac{1}{m}\sum c(H(x),y)\\c(H(x),y) =\begin{cases}
    -log(H(x)) &\text{if } y=1 \\
    -log(1-H(x)) &\text{if } y=0
    \end{cases} \\cost(W)=-\frac{1}{m}\sum ylog(H(x))+(1-y)(log(1-H(x))```$$

    costëŠ” ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ê°€ ì‘ìœ¼ë©´ ì‘ì•„ì§€ê³ , ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ê°€ í¬ë©´ ì»¤ì§€ëŠ” í•¨ìˆ˜ì´ë‹¤. c(H(x),y)ê°€ ì™œ ì €ë ‡ê²Œ ì‘ì„±ë˜ì—ˆëŠ”ì§€ ê°ê° caseì— ë”°ë¼ ìƒê°í•´ë³´ë©´ ì´í•´í•˜ê¸°ê°€ ì‰½ë‹¤. ì´ëŸ¬í•œ í•¨ìˆ˜ë¥¼ BCE(Binary Cross Entropy)ë¼ê³ í•˜ê³  ì´ì „ ì„ í˜•íšŒê·€ì—ì„œ ì‚¬ìš©í–ˆë˜ CostëŠ” MSE(Mean Squared Error)ë¼ê³  í•œë‹¤.

- ì‹¤ì œ outputì´ 1ì¼ë•Œ

    -log(x)ì˜ ê·¸ë˜í”„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ì‹¤ì œ outputì´ 1ì´ë¯€ë¡œ ì˜ˆì¸¡ê°’ì´ 1ì´ë©´ ì •ë‹µê³¼ ê°™ìœ¼ë¯€ë¡œ ë§¤ìš° ì‘ì€ ê°’, ì˜ˆì¸¡ê°’ì´ 0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ë§¤ìš° í°ê°’ìœ¼ë¡œ ê°€ì•¼í•˜ë¯€ë¡œ -log(H(x))ëŠ” Costí•¨ìˆ˜ë¡œì¨ ì˜ ì‘ë™í•˜ëŠ”ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

    ![image/output1.png](image/output1.png)

- ì‹¤ì œ outputì´ 0ì¼ ë•Œ

    -log(1-x)ì˜ ê·¸ë˜í”„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ì‹¤ì œ outputì´ 0ì´ë¯€ë¡œ ì˜ˆì¸¡ê°’ì´ 0ì´ë©´ ë§¤ìš° ì‘ì€ê°’, ì˜ˆì¸¡ê°’ì´ 1ì´ë©´ ë§¤ìš° í°ê°’ì´ ë‚˜ì™€ì•¼í•œë‹¤. ë”°ë¼ì„œ -log(1-H(x))ëŠ” costí•¨ìˆ˜ë¡œì¨ ì˜ ì‘ë™í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.

    ![image/output0.png](image/output0.png)

---

### Hypothesis code

```python
x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]
y_data = [[0],[0],[0],[1],[1],[1]]

x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)

W = torch.zeros((2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# ì§ì ‘ ìˆ˜ì‹ì„ ì½”ë“œë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.
hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W)+b))) 

# pytorchì—ëŠ” sigmoidí•¨ìˆ˜ë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ì•„ë˜ì²˜ëŸ¼ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.
torch.sigmoid(x_train.matmul(W)+b)
```

---

### Cost function code

```python
# Cost ìˆ˜ì‹ì„ ì½”ë“œë¡œ êµ¬í˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.
losses = -(y_train * torch.log(hypothesis) + 
					(1 - y_train) * torch.log(1 - hypothesis)

cost = losses.mean()

# ìœ„ì˜ ì½”ë“œëŠ” pytorchì—ì„œ ì œê³µí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ì•„ë˜ì²˜ëŸ¼ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.
F.binary_cross_entropy(hypothesis, y_train)
```

---
# [Softmax Classification](https://github.com/xcvdv/deeplearning-basicstudy/blob/main/PART1/Softmax_classification.ipynb)

### Softmax

$$P(class=i) = \frac{e^i}{\sum e^i}$$

Softmaxí•¨ìˆ˜ëŠ” ì¶œë ¥ê°’ì— ëŒ€í•œ ì •ê·œí™”ë¥¼ í•´ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤. ì˜ˆë¥¼ë“¤ì–´ íŠ¹ì • ì‚¬ì§„ì„ ë³´ê³  ê³ ì–‘ì´, ê°•ì•„ì§€, í–„ìŠ¤í„°ì¸ì§€ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ë•Œ ê³ ì–‘ì´(65%), ê°•ì•„ì§€(30%), í–„ìŠ¤í„°(5%) ì²˜ëŸ¼ ë‚˜íƒ€ë‚´ì¤€ë‹¤.

```python
import torch
import torch.nn.functional as F

z = torch.FloatTensor([1,2,3])
F.softmax(z, dim=0)
# tensor([0.0900, 0.2447, 0.6652])
```

ìµœì¢… outputì— softmaxí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ì˜ˆì¸¡í•œ ê²°ê³¼ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.

---

### cross entropy

cross entropyëŠ” ë‘ê°œì˜ í™•ë¥ ë¶„í¬ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë‘ í™•ë¥ ë¶„í¬ê°€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

$$H(P,Q)=-ğ”¼_{x\text{\textasciitilde}P(x)}[logQ(x)]=-\sum_{x\in X}P(x)logQ(x)$$

ìœ„ì˜ ì‹ì„ ì‚´í´ë³´ë©´ í™•ë¥ ë¶„í¬ Pì—ì„œ xë¥¼ ìƒ˜í”Œë§í•˜ê³  ìƒ˜í”Œë§í•œ xë¥¼ Qì— ë„£ì–´ logë¥¼ ì”Œìš´ê°’ì˜ í‰ê· ì„ êµ¬í•˜ëŠ”ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 

![image/cross.png](image/cross.png)

ë”°ë¼ì„œ cross entropyë¥¼ ìµœì†Œí™” í•˜ë„ë¡í•˜ë©´ Q2â†’Q1â†’Pì²˜ëŸ¼ ì ì  Pì— ê·¼ì‚¬í•˜ê²Œ í•  ìˆ˜ ìˆë‹¤.

ì´ë ‡ê²Œ Corss entropyë¥¼ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ ëª¨ë¸ì˜ í™•ë¥ ë¶„í¬ë¥¼ Pì— ê·¼ì‚¬í•˜ê²Œ í•  ìˆ˜ ìˆë‹¤.

---

### Cross entropy Loss

$$L = \frac{1}{N}\sum -ylog(\hat{y})$$

ìœ„ì˜ ì‹ì—ì„œ yëŠ” P(x) $,\hat{y}$ì€ Q(x)ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.  í˜¹ì€ $\hat{y}$ë¥¼ íŠ¹ì • $\theta$ê°€ ì£¼ì–´ì¡Œì„ ë•Œì˜ í™•ë¥ ë¶„í¬ $P_{\theta}(x)$ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

---

### codeë¡œ ì‘ì„±í•´ë³´ê¸°

```python
z = torch.rand(3, 5, requires_grad=True)  
y = torch.randint(5, (3,)).long()

hypothesis = F.softmax(z, dim=1) # ì˜ˆì¸¡ê°’
```

ì‹¤ì œ ê²°ê³¼ yì™€ ì˜ˆì¸¡ê²°ê³¼ë¥¼ ì‚¬ìš©í•´ Costë¥¼ êµ¬í•´ì•¼í•œë‹¤. ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ë‚˜ì™€ì•¼í•˜ëŠ” ê²°ê³¼ yë¥¼ ì‚¬ìš©í•´ Costë¥¼ êµ¬í•´ì•¼í•œë‹¤.

```python
y_one_hot = torch.zeros_like(hypothesis)
y_one_hot.scatter_(1, y.unsqueeze(1), 1)  
cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()
```

ìœ„ì˜ ì½”ë“œì—ì„œëŠ” Classì˜ ê°œìˆ˜ê°€ 5ê°œ(ë¶„ë¥˜í•´ì•¼í•  ì¢…ë¥˜ê°€ 5ê°œ)ì´ê³  inputì´ 3ê°œë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.

ë”°ë¼ì„œ yëŠ”ë¥¼ 0~5ì‚¬ì´ë¥¼ ê°€ì§€ëŠ” í¬ê¸° 3ì˜ ë°°ì—´ë¡œ ë§Œë“ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ìš°ë¦¬ì˜ ì˜ˆì¸¡ê°’ì€ 3 * 5ê°€ ëœë‹¤.

ê³„ì‚°ì„ ìœ„í•´ì„œ ì •ë‹µë§Œ 1 ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” y_one_hot ë°°ì—´ì„ ë§Œë“¤ì–´ì•¼í•œë‹¤.

scatterì€ scatter(ì¶•, ìƒˆë¡œ ë‚˜íƒ€ë‚¼ index, ìƒˆë¡œ ì €ì¥í•  ê°’)ì²˜ëŸ¼ ì‚¬ìš©í•˜ëŠ”ë° underscoerë¥¼ ì‚¬ìš©í•´ inplaceì—°ì‚°ì„ í•˜ë„ë¡ í•˜ì˜€ë‹¤.  yì—ëŠ” ê° ì…ë ¥ë³„ ëª‡ë²ˆì§¸ classê°€ ì •ë‹µì¸ì§€ê°€ ë“¤ì–´ìˆê¸° ë•Œë¬¸ì— í•œë‹¤.
ìš°ì„  zeros_likeí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ y_one_hotì„ ê°™ì€ ëª¨ì–‘ì˜ 0ìœ¼ë¡œ ì±„ì›Œì§„ ë°°ì—´ì„ ë§Œë“¤ê³ 
ì •ë‹µ indexê°€ ë“¤ì–´ìˆëŠ” yë¥¼ unsqueezeí•´ì£¼ì–´ [[1],[2],[3]]ê³¼ ê°™ì€ ëª¨ì–‘ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì–´
scatterë¥¼ ì‚¬ìš©í•´ ê° y_one_hotì˜ ê° í–‰ë³„ë¡œ y.unsqueeze()ì˜ ê°™ì€ í–‰ì— ë“¤ì–´ìˆëŠ” indexì˜ ìœ„ì¹˜ì— 1ì„ ë„£ì–´ì£¼ì—ˆë‹¤.

---

### pytorchì—ì„œ ì œê³µí•˜ëŠ” í•¨ìˆ˜ë¡œ ì‘ì„±í•˜ê¸°

ìœ„ì—ì„œëŠ” softmaxë¥¼ ì·¨í•˜ê³ , ê±°ê¸°ì— ë‹¤ì‹œ ë¡œê·¸í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì˜€ëŠ”ë° pytorchì—ëŠ” log_softmaxí•¨ìˆ˜ê°€ ìˆë‹¤.

```python
# Low level
torch.log(F.softmax(z, dim=1))
# High level
F.log_softmax(z, dim=1)
```

nll_lossë¥¼ ì‚¬ìš©í•´ Costë¥¼ ë” ì‰½ê²Œ êµ¬í•  ìˆ˜ ì´ìˆë‹¤.

nllì€ Negative Log Likelihoodì˜ ì•½ìì´ë‹¤.

```python
# Low level
(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()

# High level
F.nll_loss(F.log_softmax(z, dim=1),y)

# High levle 2
F.cross_entropy(z,y)
```

---