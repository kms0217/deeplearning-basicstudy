{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "relative-zambia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 hypothesis : tensor([0., 0., 0., 0., 0.]) Cost : 29661.80078125\n",
      "Epoch    1/100 hypothesis : tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost : 9298.5205078125\n",
      "Epoch    2/100 hypothesis : tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost : 2915.712646484375\n",
      "Epoch    3/100 hypothesis : tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost : 915.04052734375\n",
      "Epoch    4/100 hypothesis : tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]) Cost : 287.9360046386719\n",
      "Epoch    5/100 hypothesis : tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost : 91.37101745605469\n",
      "Epoch    6/100 hypothesis : tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]) Cost : 29.75813865661621\n",
      "Epoch    7/100 hypothesis : tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) Cost : 10.445304870605469\n",
      "Epoch    8/100 hypothesis : tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost : 4.391228199005127\n",
      "Epoch    9/100 hypothesis : tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost : 2.493135452270508\n",
      "Epoch   10/100 hypothesis : tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost : 1.8976876735687256\n",
      "Epoch   11/100 hypothesis : tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) Cost : 1.7105414867401123\n",
      "Epoch   12/100 hypothesis : tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost : 1.6514126062393188\n",
      "Epoch   13/100 hypothesis : tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost : 1.6323869228363037\n",
      "Epoch   14/100 hypothesis : tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost : 1.6259229183197021\n",
      "Epoch   15/100 hypothesis : tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost : 1.6234121322631836\n",
      "Epoch   16/100 hypothesis : tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost : 1.6221405267715454\n",
      "Epoch   17/100 hypothesis : tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost : 1.6212533712387085\n",
      "Epoch   18/100 hypothesis : tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) Cost : 1.620499610900879\n",
      "Epoch   19/100 hypothesis : tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) Cost : 1.6197700500488281\n",
      "Epoch   20/100 hypothesis : tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000]) Cost : 1.619032621383667\n",
      "Epoch   21/100 hypothesis : tensor([152.8022, 183.6741, 180.9683, 197.0706, 140.1009]) Cost : 1.6183456182479858\n",
      "Epoch   22/100 hypothesis : tensor([152.8021, 183.6749, 180.9686, 197.0709, 140.1016]) Cost : 1.617637038230896\n",
      "Epoch   23/100 hypothesis : tensor([152.8019, 183.6754, 180.9687, 197.0710, 140.1022]) Cost : 1.6169300079345703\n",
      "Epoch   24/100 hypothesis : tensor([152.8016, 183.6758, 180.9687, 197.0711, 140.1027]) Cost : 1.6162210702896118\n",
      "Epoch   25/100 hypothesis : tensor([152.8012, 183.6762, 180.9686, 197.0710, 140.1032]) Cost : 1.61550772190094\n",
      "Epoch   26/100 hypothesis : tensor([152.8008, 183.6765, 180.9686, 197.0710, 140.1036]) Cost : 1.6148147583007812\n",
      "Epoch   27/100 hypothesis : tensor([152.8004, 183.6768, 180.9684, 197.0709, 140.1041]) Cost : 1.614108681678772\n",
      "Epoch   28/100 hypothesis : tensor([152.8000, 183.6772, 180.9683, 197.0707, 140.1045]) Cost : 1.613386869430542\n",
      "Epoch   29/100 hypothesis : tensor([152.7995, 183.6775, 180.9682, 197.0706, 140.1049]) Cost : 1.612701416015625\n",
      "Epoch   30/100 hypothesis : tensor([152.7991, 183.6778, 180.9681, 197.0705, 140.1053]) Cost : 1.611986517906189\n",
      "Epoch   31/100 hypothesis : tensor([152.7987, 183.6781, 180.9679, 197.0704, 140.1057]) Cost : 1.6112890243530273\n",
      "Epoch   32/100 hypothesis : tensor([152.7982, 183.6784, 180.9678, 197.0703, 140.1061]) Cost : 1.6105884313583374\n",
      "Epoch   33/100 hypothesis : tensor([152.7978, 183.6787, 180.9677, 197.0702, 140.1065]) Cost : 1.6098743677139282\n",
      "Epoch   34/100 hypothesis : tensor([152.7974, 183.6790, 180.9676, 197.0701, 140.1069]) Cost : 1.6091830730438232\n",
      "Epoch   35/100 hypothesis : tensor([152.7969, 183.6793, 180.9674, 197.0700, 140.1073]) Cost : 1.608465552330017\n",
      "Epoch   36/100 hypothesis : tensor([152.7965, 183.6796, 180.9673, 197.0699, 140.1078]) Cost : 1.6077759265899658\n",
      "Epoch   37/100 hypothesis : tensor([152.7961, 183.6799, 180.9672, 197.0698, 140.1082]) Cost : 1.6070804595947266\n",
      "Epoch   38/100 hypothesis : tensor([152.7957, 183.6802, 180.9670, 197.0697, 140.1086]) Cost : 1.606384038925171\n",
      "Epoch   39/100 hypothesis : tensor([152.7952, 183.6805, 180.9669, 197.0695, 140.1090]) Cost : 1.6056652069091797\n",
      "Epoch   40/100 hypothesis : tensor([152.7948, 183.6807, 180.9668, 197.0694, 140.1094]) Cost : 1.6049741506576538\n",
      "Epoch   41/100 hypothesis : tensor([152.7944, 183.6810, 180.9666, 197.0693, 140.1098]) Cost : 1.6042665243148804\n",
      "Epoch   42/100 hypothesis : tensor([152.7939, 183.6813, 180.9665, 197.0692, 140.1102]) Cost : 1.6035856008529663\n",
      "Epoch   43/100 hypothesis : tensor([152.7935, 183.6816, 180.9664, 197.0691, 140.1106]) Cost : 1.602878212928772\n",
      "Epoch   44/100 hypothesis : tensor([152.7931, 183.6819, 180.9663, 197.0690, 140.1110]) Cost : 1.602178931236267\n",
      "Epoch   45/100 hypothesis : tensor([152.7926, 183.6822, 180.9661, 197.0689, 140.1114]) Cost : 1.6014859676361084\n",
      "Epoch   46/100 hypothesis : tensor([152.7922, 183.6826, 180.9660, 197.0688, 140.1118]) Cost : 1.6007652282714844\n",
      "Epoch   47/100 hypothesis : tensor([152.7918, 183.6828, 180.9659, 197.0686, 140.1122]) Cost : 1.6000722646713257\n",
      "Epoch   48/100 hypothesis : tensor([152.7914, 183.6831, 180.9657, 197.0685, 140.1126]) Cost : 1.5993688106536865\n",
      "Epoch   49/100 hypothesis : tensor([152.7909, 183.6834, 180.9656, 197.0684, 140.1130]) Cost : 1.598691701889038\n",
      "Epoch   50/100 hypothesis : tensor([152.7905, 183.6837, 180.9655, 197.0683, 140.1134]) Cost : 1.5979913473129272\n",
      "Epoch   51/100 hypothesis : tensor([152.7900, 183.6840, 180.9653, 197.0682, 140.1138]) Cost : 1.5972893238067627\n",
      "Epoch   52/100 hypothesis : tensor([152.7896, 183.6843, 180.9652, 197.0681, 140.1143]) Cost : 1.5965895652770996\n",
      "Epoch   53/100 hypothesis : tensor([152.7892, 183.6846, 180.9651, 197.0679, 140.1147]) Cost : 1.5958976745605469\n",
      "Epoch   54/100 hypothesis : tensor([152.7888, 183.6849, 180.9650, 197.0678, 140.1151]) Cost : 1.5951979160308838\n",
      "Epoch   55/100 hypothesis : tensor([152.7883, 183.6852, 180.9648, 197.0677, 140.1155]) Cost : 1.5945138931274414\n",
      "Epoch   56/100 hypothesis : tensor([152.7879, 183.6855, 180.9647, 197.0676, 140.1159]) Cost : 1.5938212871551514\n",
      "Epoch   57/100 hypothesis : tensor([152.7875, 183.6858, 180.9646, 197.0675, 140.1163]) Cost : 1.5931285619735718\n",
      "Epoch   58/100 hypothesis : tensor([152.7870, 183.6861, 180.9644, 197.0674, 140.1167]) Cost : 1.5924328565597534\n",
      "Epoch   59/100 hypothesis : tensor([152.7866, 183.6864, 180.9643, 197.0673, 140.1171]) Cost : 1.5917365550994873\n",
      "Epoch   60/100 hypothesis : tensor([152.7862, 183.6867, 180.9642, 197.0672, 140.1175]) Cost : 1.5910379886627197\n",
      "Epoch   61/100 hypothesis : tensor([152.7858, 183.6870, 180.9641, 197.0670, 140.1179]) Cost : 1.5903360843658447\n",
      "Epoch   62/100 hypothesis : tensor([152.7853, 183.6873, 180.9639, 197.0669, 140.1183]) Cost : 1.5896646976470947\n",
      "Epoch   63/100 hypothesis : tensor([152.7849, 183.6876, 180.9638, 197.0668, 140.1187]) Cost : 1.5889627933502197\n",
      "Epoch   64/100 hypothesis : tensor([152.7845, 183.6879, 180.9637, 197.0667, 140.1191]) Cost : 1.588273048400879\n",
      "Epoch   65/100 hypothesis : tensor([152.7840, 183.6882, 180.9635, 197.0666, 140.1195]) Cost : 1.5875762701034546\n",
      "Epoch   66/100 hypothesis : tensor([152.7836, 183.6885, 180.9634, 197.0665, 140.1199]) Cost : 1.586890459060669\n",
      "Epoch   67/100 hypothesis : tensor([152.7832, 183.6888, 180.9633, 197.0664, 140.1203]) Cost : 1.5861961841583252\n",
      "Epoch   68/100 hypothesis : tensor([152.7828, 183.6890, 180.9632, 197.0663, 140.1207]) Cost : 1.585521936416626\n",
      "Epoch   69/100 hypothesis : tensor([152.7823, 183.6893, 180.9630, 197.0661, 140.1211]) Cost : 1.5848214626312256\n",
      "Epoch   70/100 hypothesis : tensor([152.7819, 183.6896, 180.9629, 197.0660, 140.1215]) Cost : 1.5841312408447266\n",
      "Epoch   71/100 hypothesis : tensor([152.7815, 183.6899, 180.9628, 197.0659, 140.1219]) Cost : 1.5834438800811768\n",
      "Epoch   72/100 hypothesis : tensor([152.7810, 183.6902, 180.9626, 197.0658, 140.1223]) Cost : 1.582749843597412\n",
      "Epoch   73/100 hypothesis : tensor([152.7806, 183.6905, 180.9625, 197.0657, 140.1227]) Cost : 1.5820715427398682\n",
      "Epoch   74/100 hypothesis : tensor([152.7802, 183.6908, 180.9624, 197.0656, 140.1231]) Cost : 1.5813846588134766\n",
      "Epoch   75/100 hypothesis : tensor([152.7798, 183.6911, 180.9622, 197.0655, 140.1236]) Cost : 1.5806901454925537\n",
      "Epoch   76/100 hypothesis : tensor([152.7793, 183.6914, 180.9621, 197.0654, 140.1240]) Cost : 1.5799963474273682\n",
      "Epoch   77/100 hypothesis : tensor([152.7789, 183.6917, 180.9620, 197.0653, 140.1244]) Cost : 1.5792994499206543\n",
      "Epoch   78/100 hypothesis : tensor([152.7785, 183.6920, 180.9619, 197.0651, 140.1248]) Cost : 1.5786316394805908\n",
      "Epoch   79/100 hypothesis : tensor([152.7781, 183.6923, 180.9617, 197.0650, 140.1252]) Cost : 1.5779469013214111\n",
      "Epoch   80/100 hypothesis : tensor([152.7776, 183.6926, 180.9616, 197.0649, 140.1256]) Cost : 1.5772708654403687\n",
      "Epoch   81/100 hypothesis : tensor([152.7772, 183.6929, 180.9615, 197.0648, 140.1260]) Cost : 1.5765583515167236\n",
      "Epoch   82/100 hypothesis : tensor([152.7768, 183.6932, 180.9614, 197.0647, 140.1264]) Cost : 1.5758955478668213\n",
      "Epoch   83/100 hypothesis : tensor([152.7764, 183.6935, 180.9612, 197.0646, 140.1268]) Cost : 1.5752052068710327\n",
      "Epoch   84/100 hypothesis : tensor([152.7759, 183.6938, 180.9611, 197.0645, 140.1272]) Cost : 1.574519157409668\n",
      "Epoch   85/100 hypothesis : tensor([152.7755, 183.6940, 180.9610, 197.0643, 140.1276]) Cost : 1.5738390684127808\n",
      "Epoch   86/100 hypothesis : tensor([152.7751, 183.6944, 180.9609, 197.0643, 140.1280]) Cost : 1.573149561882019\n",
      "Epoch   87/100 hypothesis : tensor([152.7747, 183.6947, 180.9607, 197.0641, 140.1284]) Cost : 1.5724574327468872\n",
      "Epoch   88/100 hypothesis : tensor([152.7742, 183.6949, 180.9606, 197.0640, 140.1288]) Cost : 1.5717909336090088\n",
      "Epoch   89/100 hypothesis : tensor([152.7738, 183.6952, 180.9605, 197.0639, 140.1292]) Cost : 1.5711135864257812\n",
      "Epoch   90/100 hypothesis : tensor([152.7734, 183.6955, 180.9603, 197.0638, 140.1296]) Cost : 1.5704349279403687\n",
      "Epoch   91/100 hypothesis : tensor([152.7729, 183.6958, 180.9602, 197.0637, 140.1300]) Cost : 1.5697546005249023\n",
      "Epoch   92/100 hypothesis : tensor([152.7725, 183.6961, 180.9601, 197.0636, 140.1304]) Cost : 1.5690643787384033\n",
      "Epoch   93/100 hypothesis : tensor([152.7721, 183.6964, 180.9600, 197.0634, 140.1308]) Cost : 1.5683857202529907\n",
      "Epoch   94/100 hypothesis : tensor([152.7717, 183.6967, 180.9598, 197.0633, 140.1312]) Cost : 1.5677077770233154\n",
      "Epoch   95/100 hypothesis : tensor([152.7713, 183.6970, 180.9597, 197.0632, 140.1316]) Cost : 1.567021369934082\n",
      "Epoch   96/100 hypothesis : tensor([152.7708, 183.6973, 180.9596, 197.0631, 140.1320]) Cost : 1.5663396120071411\n",
      "Epoch   97/100 hypothesis : tensor([152.7704, 183.6976, 180.9594, 197.0630, 140.1324]) Cost : 1.5656578540802002\n",
      "Epoch   98/100 hypothesis : tensor([152.7700, 183.6979, 180.9593, 197.0629, 140.1328]) Cost : 1.5649865865707397\n",
      "Epoch   99/100 hypothesis : tensor([152.7695, 183.6982, 180.9592, 197.0628, 140.1332]) Cost : 1.5642985105514526\n",
      "Epoch  100/100 hypothesis : tensor([152.7691, 183.6985, 180.9591, 197.0627, 140.1336]) Cost : 1.5636341571807861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[152.7687],\n",
       "        [183.6987],\n",
       "        [180.9589],\n",
       "        [197.0626],\n",
       "        [140.1340]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],[93,88,93],[89,91,90],[96,98,100],[73,66,70]])\n",
    "\n",
    "# 학생 5명의 기말고사 점수\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "W = torch.zeros((3,1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "optimizer = optim.SGD([W,b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch:4d}/{nb_epochs} hypothesis : {hypothesis.squeeze().detach()} Cost : {cost.item()}\")\n",
    "\n",
    "hypothesis = x_train.matmul(W) + b\n",
    "hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-greenhouse",
   "metadata": {},
   "source": [
    "### MultiLinearRegression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occupational-meeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis : tensor([17.5838, 21.0816, 20.7812, 22.8309, 15.9048]) Cost : 23235.01953125\n",
      "Epoch    1/20 hypothesis : tensor([77.1101, 92.6287, 91.2773, 99.5994, 70.4774]) Cost : 7284.2421875\n",
      "Epoch    2/20 hypothesis : tensor([110.4365, 132.6855, 130.7455, 142.5792, 101.0308]) Cost : 2284.518798828125\n",
      "Epoch    3/20 hypothesis : tensor([129.0946, 155.1119, 152.8421, 166.6420, 118.1367]) Cost : 717.37109375\n",
      "Epoch    4/20 hypothesis : tensor([139.5403, 167.6678, 165.2132, 180.1138, 127.7139]) Cost : 226.1525421142578\n",
      "Epoch    5/20 hypothesis : tensor([145.3883, 174.6975, 172.1393, 187.6561, 133.0760]) Cost : 72.18128204345703\n",
      "Epoch    6/20 hypothesis : tensor([148.6622, 178.6333, 176.0168, 191.8787, 136.0782]) Cost : 23.918806076049805\n",
      "Epoch    7/20 hypothesis : tensor([150.4949, 180.8370, 178.1877, 194.2428, 137.7593]) Cost : 8.790411949157715\n",
      "Epoch    8/20 hypothesis : tensor([151.5207, 182.0708, 179.4030, 195.5663, 138.7006]) Cost : 4.048011302947998\n",
      "Epoch    9/20 hypothesis : tensor([152.0949, 182.7618, 180.0834, 196.3072, 139.2278]) Cost : 2.5609607696533203\n",
      "Epoch   10/20 hypothesis : tensor([152.4161, 183.1488, 180.4642, 196.7219, 139.5232]) Cost : 2.094296932220459\n",
      "Epoch   11/20 hypothesis : tensor([152.5958, 183.3656, 180.6774, 196.9541, 139.6888]) Cost : 1.9474643468856812\n",
      "Epoch   12/20 hypothesis : tensor([152.6962, 183.4871, 180.7967, 197.0840, 139.7816]) Cost : 1.9008781909942627\n",
      "Epoch   13/20 hypothesis : tensor([152.7521, 183.5552, 180.8634, 197.1567, 139.8338]) Cost : 1.885724425315857\n",
      "Epoch   14/20 hypothesis : tensor([152.7833, 183.5935, 180.9006, 197.1973, 139.8632]) Cost : 1.880422830581665\n",
      "Epoch   15/20 hypothesis : tensor([152.8005, 183.6151, 180.9215, 197.2200, 139.8799]) Cost : 1.878204345703125\n",
      "Epoch   16/20 hypothesis : tensor([152.8100, 183.6273, 180.9331, 197.2327, 139.8894]) Cost : 1.8769718408584595\n",
      "Epoch   17/20 hypothesis : tensor([152.8151, 183.6344, 180.9395, 197.2397, 139.8949]) Cost : 1.8760244846343994\n",
      "Epoch   18/20 hypothesis : tensor([152.8177, 183.6384, 180.9430, 197.2436, 139.8982]) Cost : 1.8751652240753174\n",
      "Epoch   19/20 hypothesis : tensor([152.8190, 183.6408, 180.9450, 197.2457, 139.9003]) Cost : 1.874366044998169\n",
      "Epoch   20/20 hypothesis : tensor([152.8195, 183.6423, 180.9460, 197.2468, 139.9016]) Cost : 1.8735603094100952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[152.8196],\n",
       "        [183.6433],\n",
       "        [180.9465],\n",
       "        [197.2474],\n",
       "        [139.9025]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 클래스 정의\n",
    "class MultiLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],[93,88,93],[89,91,90],[96,98,100],[73,66,70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "# 모델 정의\n",
    "model = MultiLinearRegression()\n",
    "\n",
    "# optimizer 정의 -> 학습을 통화 변화할 값은 model.parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # Hypothesis 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # troch.nn.functional에서 제공하는 loss function을 사용가능하다.\n",
    "    # 장점 : 다른 loss와 쉽게 교체 가능\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # cost로 개선 (gradient descent)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch:4d}/{nb_epochs} hypothesis : {prediction.squeeze().detach()} Cost : {cost.item()}\")\n",
    "\n",
    "model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
